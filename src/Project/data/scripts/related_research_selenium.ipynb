{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Related Scientific Work Search </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is aimed at creating document index ```(AnnoyIndex)``` using data crawled from scientific articles databases. Tool used to retrieve the data is Selenium. The Abstract, Link and year of publication is retrieved and stored. To reitrieve scientific work related to a given query, a Doc 2 Vec model is used to transform the crawled document abstact text to vectors to later create an Annoy tree.\n",
    "\n",
    "### Objective\n",
    "1. Search for most relavent scientific research paper based on query \n",
    "1. Spellcheck query (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, selenium, requests, re , pickle\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an Abstract Data Crawler\n",
    "This class serves as a template to implement a specific scientific articles base crwaler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCrawler(metaclass=ABCMeta):\n",
    "    \"\"\"Base searcher to be used for all academic databases crawlers.\"\"\"\n",
    "    def __init__(self, output_directory, base_url):\n",
    "        self.output_directory = output_directory\n",
    "        self.base_url = base_url\n",
    "        self.results = {}\n",
    "    \n",
    "    @abstractmethod\n",
    "    def search(self):\n",
    "        \"\"\"Search a given query using base_url\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def filter_by_year(self):\n",
    "        \"\"\"Processes raw data. This step should create the raw dataframe with all the required features. Shouldn't implement statistical or text cleaning.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        \"\"\"Saves processed data.\"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IEEEXplore Crawler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IEEESeach(DataCrawler):\n",
    "    \"\"\"IEEE document base searcher\"\"\"\n",
    "    def __init__(self,output_dir):\n",
    "        self.url = \"https://ieeexplore.ieee.org\"\n",
    "        self.browser = webdriver.Chrome(executable_path='./chromedriver')\n",
    "        super().__init__(os.path.join(output_dir, 'IEEE'),self.url)\n",
    "    \n",
    "    def search(self,query=\"A Lightweight Autoencoder\"):\n",
    "        \"\"\"finds documents related to query in IEEE document base\"\"\"\n",
    "        \n",
    "        self.browser.get(\"https://ieeexplore.ieee.org\")\n",
    "        self.browser.implicitly_wait(10) #wait 10 sec for website to load \n",
    "        input_element = self.browser.find_element_by_class_name(\"Typeahead-input\") #find the query input field\n",
    "        input_element.send_keys(query) #pass the query to input field\n",
    "        \n",
    "        action = ActionChains(self.browser).send_keys(Keys.ENTER)\n",
    "        action.perform() #press search button\n",
    "        \n",
    "        #get all the urls of the articles/documents found\n",
    "        for item in self.browser.find_elements_by_class_name(\"List-results-items\"):\n",
    "            text = item.text.split(\"\\n\")\n",
    "            title = text[0].strip()\n",
    "            year = text[3].split(\"|\")[0].split(\":\")[1].strip()\n",
    "    \n",
    "            #Get document link \n",
    "            doc_id = str(hex(time.time().as_integer_ratio()[0]))\n",
    "            link = item.find_element_by_tag_name(\"a\").get_attribute(\"href\")\n",
    "            self.results[doc_id] = {\"year\" : int(year), \"link\":link}\n",
    "            self.results[doc_id]['title'] = title\n",
    "            \n",
    "            \n",
    "        \n",
    "        #retrieve abstract text for each article/document in the results\n",
    "        for name, doc_info in self.results.items():\n",
    "            self.browser.get(doc_info['link'])\n",
    "            time.sleep(0.05)\n",
    "            abstract_text = self.browser.find_element_by_class_name(\"abstract-text\").text\n",
    "            self.results[name].update({\"Abstract\":abstract_text.split(\"\\n\")[1]})\n",
    "        \n",
    "        self.browser.quit()\n",
    "            \n",
    "    def filter_by_year(self):\n",
    "        \"\"\"extract any text if any .pdf, \"\"\"\n",
    "        print(\"Coming soon!!\")\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Saves processed data.\"\"\"\n",
    "        print(\"Saving....\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ieee_docs = IEEESeach(\"./\")\n",
    "ieee_docs.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. arXiv data crawler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arXiv(DataCrawler):\n",
    "    \"\"\"arXiv.org document base searcher\"\"\"\n",
    "    def __init__(self,output_dir):\n",
    "        self.url = \"https://arxiv.org\"\n",
    "        self.browser = webdriver.Chrome(executable_path='./chromedriver')\n",
    "        super().__init__(os.path.join(output_dir, 'arXiv'),self.url)\n",
    "    \n",
    "    def search(self,query=\"A Lightweight Autoencoder\"):\n",
    "        \"\"\"finds documents related to query in arXiv document base\"\"\"\n",
    "        \n",
    "        self.browser.get(self.url)\n",
    "        self.browser.implicitly_wait(10) #wait 10 sec for website to load \n",
    "        input_element = self.browser.find_element_by_name(\"query\") #find the query input field\n",
    "        input_element.send_keys(query) #pass the query to input field\n",
    "        \n",
    "        action = ActionChains(self.browser).send_keys(Keys.ENTER)\n",
    "        action.perform() #press search button\n",
    "        \n",
    "        #expand the result div to show full abstract text\n",
    "        expand = self.browser.find_elements_by_partial_link_text('â–½ More')\n",
    "        for x in range(0,len(expand)):\n",
    "            expand[x].click()\n",
    "        \n",
    "        #get all the urls of the articles/documents found\n",
    "        res_art = self.browser.find_elements_by_class_name(\"arxiv-result\")\n",
    "        self.results = {}\n",
    "        for r in res_art:\n",
    "            doc_id = str(hex(time.time().as_integer_ratio()[0]))\n",
    "            title = r.find_element_by_class_name(\"title\").text\n",
    "            abstract = r.find_element_by_class_name(\"abstract\").text[10:-6]\n",
    "            pdf_link = r.find_element_by_partial_link_text(\"pdf\").get_attribute(\"href\")\n",
    "            dates = r.find_element_by_css_selector(\"p.is-size-7\").text\n",
    "            self.results[doc_id] = {\"title\": title, \"year\": dates, \"link\": pdf_link, \"Abstract\":abstract}\n",
    "\n",
    "        self.browser.quit()\n",
    "            \n",
    "    def filter_by_year(self):\n",
    "        \"\"\"extract any text if any .pdf, \"\"\"\n",
    "        print(\"Coming soon!!\")\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Saves processed data.\"\"\"\n",
    "        print(\"Saving....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arXiv_docs = arXiv(\"./\")\n",
    "arXiv_docs.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ScienceDirect Data crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScienceDirect(DataCrawler):\n",
    "    \"\"\"sciencedirect.com document base searcher\"\"\"\n",
    "    def __init__(self,output_dir):\n",
    "        self.url = \"https://www.sciencedirect.com\"\n",
    "        self.browser = webdriver.Chrome(executable_path='./chromedriver')\n",
    "        super().__init__(os.path.join(output_dir, 'sciencedirect'),self.url)\n",
    "    \n",
    "    def search(self,query=\"A Lightweight Autoencoder\"):\n",
    "        \"\"\"finds documents related to query in science direct document base\"\"\"\n",
    "        \n",
    "        self.browser.get(self.url)\n",
    "        self.browser.implicitly_wait(10) #wait 10 sec for website to load \n",
    "        input_element = self.browser.find_element_by_name(\"qs\") #find the query input field\n",
    "        input_element.send_keys(query) #pass the query to input field\n",
    "        \n",
    "        action = ActionChains(self.browser).send_keys(Keys.ENTER)\n",
    "        action.perform() #press search button\n",
    "        \n",
    "        #expand the result div to show full abstract text\n",
    "        self.results = {}\n",
    "        time.sleep(0.01)\n",
    "        res_items = self.browser.find_elements_by_class_name(\"ResultItem\")\n",
    "        time.sleep(0.01)\n",
    "        for i in res_items:\n",
    "            wait = WebDriverWait(i, 10)\n",
    "            i.find_element_by_css_selector(\"[aria-label=Abstract]\").click()\n",
    "            element = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'preview-body-container')))\n",
    "            \n",
    "            abstract = element.find_element_by_tag_name(\"p\").text\n",
    "            title = i.find_element_by_class_name(\"result-list-title-link\").text\n",
    "            pdf_link = i.find_element_by_partial_link_text(\"Download PDF\").get_attribute(\"href\")\n",
    "            \n",
    "            date = \" \".join([j.text for j in i.find_element_by_class_name(\"SubType\").find_elements_by_tag_name('span')])\n",
    "            date = re.findall('(?:January|February|March|April|May|June|July|August|September|October|November|December)[\\s-]\\d{2,4}', date)\n",
    "            \n",
    "            date = date[0] if len(date)> 0 else 'Unknown'\n",
    "            doc_id = str(hex(time.time().as_integer_ratio()[0]))\n",
    "            self.results[doc_id] = { \"year\": date, \"Abstract\":abstract, \"link\": pdf_link, \"title\": title }\n",
    "        \n",
    "        self.browser.quit()\n",
    "            \n",
    "    def filter_by_year(self):\n",
    "        \"\"\"extract any text if any .pdf, \"\"\"\n",
    "        print(\"Coming soon!!\")\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Saves processed data.\"\"\"\n",
    "        print(\"Saving....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScienceDirect_docs = ScienceDirect(\"./\")\n",
    "ScienceDirect_docs.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. More to come ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #mdpi\n",
    "# #Science Direct\n",
    "\n",
    "# browser = webdriver.Chrome(executable_path='./chromedriver')\n",
    "\n",
    "# browser.get(\"https://www.mdpi.com\")\n",
    "# browser.implicitly_wait(60) #wait 10 sec for website to load \n",
    "# input_element = browser.find_element_by_name(\"q\") #find the query input field\n",
    "# # input_element.send_keys(\"A lightweight autoencoder\") #pass the query to input field\n",
    "# input_element.send_keys(\"sugar\")\n",
    "\n",
    "# action = ActionChains(browser).send_keys(Keys.ENTER)\n",
    "# action.perform() #press search button\n",
    "\n",
    "# items = browser.find_elements_by_class_name(\"article-item\")\n",
    "\n",
    "# print(items[0].find_element_by_class_name(\"title-link\").text)\n",
    "# print(items[1].find_element_by_class_name(\"abstract-full\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Join the data retrieved from different sources  & Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join all the crawled data\n",
    "all_data = {**ieee_docs.results, **arXiv_docs.results, **ScienceDirect_docs.results}\n",
    "with open('raw_data.pickle','wb') as handle:\n",
    "    pickle.dump(np.array([i for i in all_data.items()]),handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc to Vec\n",
    "Create a document to vector model for the retrieved data. Start by cleaning the data, then convert each document to ```TaggedDocument``` to be used in training the Doc2Vec model. Lastly save the model for further use in processing a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read full data from disk\n",
    "with open('joined_raw_data.pickle','rb') as handler:\n",
    "    all_data = pickle.load(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean(sentence):\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in sentence.strip().split(' ')]\n",
    "    \n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in stripped if not w in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "all_data = [clean(doc.get('Abstract')) for _ , doc in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_data)]\n",
    "\n",
    "# train a model\n",
    "model = Doc2Vec(\n",
    "    documents,     # collection of texts\n",
    "    vector_size=5, # output vector size\n",
    "    window=2,      # maximum distance between the target word and its neighboring word\n",
    "    min_count=1,   # minimal number of \n",
    "    workers=4      # in parallel\n",
    ")\n",
    "\n",
    "# clean training data and save model to disk\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index using Annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use Eiclidean distance for the index. Also multiple others allowed\n",
    "index = AnnoyIndex(5, 'euclidean')\n",
    "\n",
    "for i, row in enumerate(all_data):\n",
    "    index.add_item(i,model.infer_vector(row))\n",
    "\n",
    "index.build(20) # number of trees\n",
    "\n",
    "# Save Index to disk\n",
    "index.save('index.ann')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
