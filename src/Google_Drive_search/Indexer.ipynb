{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2. Building inverted index and answering queries\n",
    "\n",
    "In this lab you are going to implement a standard document processing pipeline and then build a simple search engine based on it: starting from crawling documents, then building an inverted index, answering queries using this index, and organizing it as a simple web server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a unified approach to documents preprocessing, and this class is responsible for it. Complete the code for given functions (most of them are just one-liners) and make sure you pass the tests. Make use of `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = {'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from', 'has', 'he', 'in', 'is', 'it', 'its',\n",
    "                      'of', 'on', 'that', 'the', 'to', 'was', 'were', 'will', 'with'}\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "\n",
    "    # word tokenize text using nltk lib\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "    # stem word using provided stemmer\n",
    "    def stem(self, word, stemmer):\n",
    "        return stemmer.stem(word)\n",
    "\n",
    "\n",
    "    # check if word is appropriate - not a stop word and isalpha, \n",
    "    # i.e consists of letters, not punctuation, numbers, dates\n",
    "    def is_apt_word(self, word):\n",
    "        return word not in self.stop_words and word.isalpha()\n",
    "\n",
    "\n",
    "    # combines all previous methods together\n",
    "    # tokenizes lowercased text and stems it, ignoring not appropriate words\n",
    "    def preprocess(self, text):\n",
    "        tokenized = self.tokenize(text.lower())\n",
    "        return [self.stem(w, self.ps) for w in tokenized if self.is_apt_word(w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Tests ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = Preprocessor()\n",
    "text = 'To be, or not to be, that is the question'\n",
    "\n",
    "assert prep.tokenize(text) == ['To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question']\n",
    "assert prep.stem('retrieval', prep.ps) == 'retriev'\n",
    "assert prep.is_apt_word('qwerty123') is False\n",
    "assert prep.preprocess(text) == ['or', 'not', 'question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Crawling and Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Base classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some base classes we will need for writing our indexer. The code from the last lab's solution is given, but note that you will need to change some of it, namely, the `parse` function. The reason is it always makes complete parsing, which we want to avoid when we only need links, for example, or a specific portion of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def download(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            if response.status_code == 200:\n",
    "                self.content = response.content\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def persist(self, path):\n",
    "        with open(os.path.join(path, quote(self.url).replace('/', '_')), 'wb') as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def normalize(self, href):\n",
    "        if href is not None and href[:4] != 'http':\n",
    "            href = urllib.parse.urljoin(self.url, href)\n",
    "        return href\n",
    "\n",
    "    def parse(self, links_only=False):\n",
    "\n",
    "        def tag_visible(element):\n",
    "            if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "                return False\n",
    "            if isinstance(element, Comment):\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "        model = BeautifulSoup(self.content, \"html.parser\")\n",
    "\n",
    "        self.anchors = []\n",
    "        a = model.find_all('a')\n",
    "        for anchor in a:\n",
    "            href = self.normalize(anchor.get('href'))\n",
    "            text = anchor.text\n",
    "            self.anchors.append((text, href))\n",
    "\n",
    "        if links_only:\n",
    "            return        \n",
    "\n",
    "        # extract only from known blocks\n",
    "        header = model.find('h1', {'class': 'ArticleHeader_headline'})\n",
    "        body = model.find('div', {'class': 'StandardArticleBody_body'})\n",
    "        if body:\n",
    "            # remove a known repeating block at the end of the article\n",
    "            last_block = body.find('div', {'class': 'StandardArticleBody_trustBadgeContainer'})\n",
    "            if last_block:\n",
    "                last_block.decompose()\n",
    "\n",
    "        # ignore broken pages\n",
    "        if header is None or body is None:\n",
    "            self.text = \"\"\n",
    "            return\n",
    "                \n",
    "        texts = header.findAll(text=True) + body.findAll(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)\n",
    "        self.text = u\" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Main class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main indexer logic is here. We organize it as a crawler generator that adds certain visited pages to inverted index and saves them on disk. \n",
    "\n",
    "- `crawl_generator_for_index` method crawles the given website doing BFS, starting from `source` within given `depth`. Considers only inner pages (of a form https://www.reuters.com/...) for visiting. To speed up, doesn't consider for visiting pages with content type other than html: '.pdf', '.mp3', '.avi', '.mp4', '.txt'. If encounters an article page (of a form https://www.reuters.com/article/...), saves its content in a file in `collection_path` folder and populates the inverted index calling `index_doc` method. When done, saves on disk three resulting dictionaries:\n",
    "    - `doc_urls`, `doc_id:url`\n",
    "    - `index`, `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
    "    - `doc_lengths`, `doc_id:doc_length` \n",
    "\n",
    "    `limit` parameter is given for testing - if not `None`, break the loop when number of saved articles exceeds the `limit` and return without writing dictionaries to disk.\n",
    "    \n",
    "    \n",
    "- `index_doc` method parses and preprocesses the content of a `doc` and adds it to the inverted index. Also keeps track of document lengths in a `doc_lengths` dictionary.\n",
    "\n",
    "\n",
    "**Bonus task \\*** In real industrial systems a crawler would pass the links to the dedicated service that would load their contents in a bunch of parallel threads. Implement such a service - get urls as inputs, load page contents in parallel and return filenames on disk, which are then processed by indexer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self):      \n",
    "        # dictionaries to populate\n",
    "        self.doc_urls = {}        \n",
    "        self.index = {}\n",
    "        self.doc_lengths = {}\n",
    "        # preprocessor\n",
    "        self.prep = Preprocessor()\n",
    "        \n",
    "    \n",
    "    def crawl_generator_for_index(self, source, depth, collection_path=\"collection\", limit=None):        \n",
    "        q = Queue()\n",
    "        q.put((source, 0))\n",
    "        visited = set()\n",
    "        doc_id = 0\n",
    "        # creating a folder if needed\n",
    "        if not os.path.exists(collection_path):\n",
    "            os.makedirs(collection_path)\n",
    "        # doing a BFS\n",
    "        while not q.empty():\n",
    "            url, url_depth = q.get()\n",
    "            if url not in visited:\n",
    "                visited.add(url)\n",
    "                try:\n",
    "                    # download and parse url\n",
    "                    doc = HtmlDocument(url)\n",
    "                    if doc.download():\n",
    "                        doc.parse(links_only=True)\n",
    "                        # save content on disk if url matches the pattern\n",
    "                        if url.startswith(\"https://www.reuters.com/article/\"):\n",
    "                            doc.persist(collection_path)\n",
    "                            self.doc_urls[doc_id] = url\n",
    "                            # add document to index\n",
    "                            self.index_doc(doc, doc_id)\n",
    "                            doc_id +=1                        \n",
    "                            if limit is not None:\n",
    "                                limit -= 1\n",
    "                                if limit == 0:\n",
    "                                    return                    \n",
    "                        # filter links, consider only inner html pages\n",
    "                        if url_depth + 1 < depth:\n",
    "                            valid_anchors = filter(self.apt_url, doc.anchors)\n",
    "                            for a in valid_anchors:\n",
    "                                q.put((a[1], url_depth + 1))\n",
    "                        yield doc\n",
    "                except FileNotFoundError as e:\n",
    "                    print(\"Analyzing\", url, \"led to FileNotFoundError\")\n",
    "        \n",
    "        # save results for later use\n",
    "        with open('doc_urls.p', 'wb') as fp:\n",
    "            pickle.dump(self.doc_urls, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('inverted_index.p', 'wb') as fp:\n",
    "            pickle.dump(self.index, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open('doc_lengths.p', 'wb') as fp:\n",
    "            pickle.dump(self.doc_lengths, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "    def apt_url(self, anchor):\n",
    "        url = str(anchor[1])\n",
    "        if not url.startswith(\"https://www.reuters.com/\"):\n",
    "            return False\n",
    "        if url[-4:]  in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "            return False\n",
    "        return True    \n",
    "    \n",
    "    \n",
    "    def index_doc(self, doc, doc_id):\n",
    "        doc.parse()\n",
    "        # preprocess - tokenize, remove stopwords and non-alphanumeric tokens and stem\n",
    "        content = self.prep.preprocess(doc.text)\n",
    "#         print(\"text\", doc.text)\n",
    "#         print(\"content\", content)        \n",
    "        self.doc_lengths[doc_id] = len(content)\n",
    "        # get dict of terms in current article\n",
    "        article_index = Counter(content)\n",
    "        # update global index\n",
    "        for term in article_index.keys():\n",
    "            article_freq = article_index[term]\n",
    "            if term not in self.index:                \n",
    "                self.index[term] = [article_freq, (doc_id, article_freq)]\n",
    "            else:\n",
    "                self.index[term][0] += article_freq\n",
    "                self.index[term].append((doc_id, article_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Tests ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.reuters.com/news/us\n",
      "2 https://www.reuters.com/\n",
      "3 https://www.reuters.com/finance\n",
      "4 https://www.reuters.com/finance/markets\n",
      "5 https://www.reuters.com/news/world\n",
      "6 https://www.reuters.com/politics\n",
      "7 https://www.reuters.com/video\n",
      "8 https://www.reuters.com/news/archive/domesticNews\n",
      "9 https://www.reuters.com/article/us-usa-blast/blast-at-machine-shop-rips-through-houston-neighborhood-killing-two-idUSKBN1ZN18J\n",
      "10 https://www.reuters.com/article/us-usa-mexico-corruption/u-s-charges-former-mexico-police-commander-in-el-chapo-linked-cocaine-probe-idUSKBN1ZN206\n",
      "11 https://www.reuters.com/news/archive/politicsNews\n",
      "12 https://www.reuters.com/article/us-usa-trump-impeachment/democrats-in-impeachment-trial-say-trump-abused-his-power-for-political-gain-idUSKBN1ZM1Q7\n",
      "13 https://www.reuters.com/news/archive/technologyNews\n",
      "14 https://www.reuters.com/article/us-internet-regulation-justice/u-s-justice-department-plans-to-hold-meeting-to-discuss-tech-industry-liability-sources-idUSKBN1ZN2E5\n"
     ]
    }
   ],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/news/us\", 2, \"test_collection\", 5):\n",
    "    print(k, c.url)\n",
    "    k+=1\n",
    "\n",
    "assert type(indexer.index) is dict\n",
    "assert type(indexer.index['reuter']) is list\n",
    "assert type(indexer.index['reuter'][0]) is int\n",
    "assert type(indexer.index['reuter'][1]) is tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Building index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer()\n",
    "k = 1\n",
    "for c in indexer.crawl_generator_for_index(\"https://www.reuters.com/\", 3, \"docs_collection\"):\n",
    "    print(k, c.url)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Index statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index, doc_lengths and doc_urls\n",
    "with open('inverted_index.p', 'rb') as fp:\n",
    "    index = pickle.load(fp)\n",
    "with open('doc_lengths.p', 'rb') as fp:\n",
    "    doc_lengths = pickle.load(fp)\n",
    "with open('doc_urls.p', 'rb') as fp:\n",
    "    doc_urls = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index length 14247\n",
      "\n",
      "Top terms by number of documents they apperared in:\n",
      "[('reuter', 717), ('edit', 675), ('s', 670), ('said', 660), ('report', 649), ('have', 509), ('year', 477), ('not', 471), ('which', 466), ('but', 465), ('thi', 461), ('new', 429), ('after', 423), ('more', 421), ('their', 383), ('had', 381), ('over', 380), ('file', 379), ('last', 376), ('who', 373)]\n",
      "\n",
      "Top terms by overall frequency:\n",
      "[('s', 3695), ('said', 3121), ('have', 1454), ('year', 1293), ('not', 1186), ('reuter', 1149), ('but', 1112), ('report', 1092), ('new', 1073), ('hi', 1046), ('thi', 970), ('which', 949), ('they', 944), ('who', 906), ('more', 876), ('after', 866), ('we', 839), ('their', 835), ('had', 767), ('compani', 758)]\n"
     ]
    }
   ],
   "source": [
    "print('Total index length', len(index))\n",
    "print('\\nTop terms by number of documents they apperared in:')\n",
    "sorted_by_n_docs = sorted(index.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=True)\n",
    "print([(sorted_by_n_docs[i][0], len(sorted_by_n_docs[i][1])) for i in range(20)])\n",
    "print('\\nTop terms by overall frequency:')\n",
    "sorted_by_freq = sorted(index.items(), key=lambda kv: (kv[1][0], kv[0]), reverse=True)\n",
    "print([(sorted_by_freq[i][0], sorted_by_freq[i][1][0]) for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Answering query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given that we already have built the inverted index, it's time to utilize it for answering user queries. In this class there are two methods you need to implement:\n",
    "- `boolean_retrieval`, the simplest form of document retrieval which returns a set of documents such that each one contains all query terms. Returns a set of document ids. Refer to *ch.1* of the book for details;\n",
    "- `okapi_scoring`, Okapi BM25 ranking function - assigns scores to documents in the collection that are relevant to the user query. Returns a dictionary of scores, `doc_id:score`. Read about it in [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) and implement accordingly.\n",
    "\n",
    "Both methods accept `query` parameter in a form of a dictionary, `term:frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class QueryProcessing:\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_query(raw_query):\n",
    "        prep = Preprocessor()\n",
    "        # pre-process query the same way as documents\n",
    "        query = prep.preprocess(raw_query)\n",
    "        # count frequency\n",
    "        return Counter(query)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boolean_retrieval(query, index):\n",
    "        postings = []\n",
    "        for term in query.keys():\n",
    "            if term not in index:  # ignoring absent terms\n",
    "                continue\n",
    "            posting = index[term][1:]\n",
    "            # extract document info only\n",
    "            posting = [i[0] for i in posting]\n",
    "            postings.append(posting)\n",
    "        docs = set.intersection(*map(set,postings))\n",
    "        return docs \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def okapi_scoring(query, doc_lengths, index, k1=1.2, b=0.75):\n",
    "        scores = {}\n",
    "        N = len(doc_lengths)\n",
    "        avgdl = sum(doc_lengths.values()) / float(len(doc_lengths))\n",
    "        for term in query.keys():\n",
    "            if term not in index:  # ignoring absent terms\n",
    "                continue\n",
    "            n_docs = len(index[term]) - 1\n",
    "            idf = math.log10((N - n_docs + 0.5) / (n_docs + 0.5))\n",
    "            postings = index[term][1:]\n",
    "            for posting in postings:\n",
    "                doc_id = posting[0]\n",
    "                doc_tf = posting[1]\n",
    "                score = idf * doc_tf * (k1 + 1) / (doc_tf + k1 * (1 - b + b * (doc_lengths[doc_id] / avgdl)))\n",
    "                if doc_id not in scores:\n",
    "                    scores[doc_id] = score\n",
    "                else:  # accumulate scores\n",
    "                    scores[doc_id] += score\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_lengths = {1: 20, 2: 15, 3: 10, 4:20, 5:30}\n",
    "test_index = {'x': [2, (1, 1), (2, 1)], 'y': [2, (1, 1), (3, 1)], 'z': [3, (2, 1), (4,2)]}\n",
    "\n",
    "\n",
    "test_query1 = QueryProcessing.prepare_query('x z')\n",
    "test_query2 = QueryProcessing.prepare_query('x y')\n",
    "\n",
    "\n",
    "assert QueryProcessing.boolean_retrieval(test_query1, test_index) == {2}\n",
    "assert QueryProcessing.boolean_retrieval(test_query2, test_index) == {1}\n",
    "okapi_res = QueryProcessing.okapi_scoring(test_query2, test_doc_lengths, test_index)\n",
    "assert all(k in okapi_res for k in (1,2,3))\n",
    "assert not any(k in okapi_res for k in (4,5))\n",
    "assert okapi_res[1] > okapi_res[3] > okapi_res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setting up a server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus task \\*** Organize the resulting search engine as a web-service that gets a query from get-parameters and returns urls with scores as a `json` dictionary. Check its work in a browser of with curl, should look smth like this:\n",
    " \n",
    "`> curl localhost:8080/?q=some_query_text\n",
    "{ \"url1\" : 0.9, \"url2\": 0.8 }`\n",
    "\n",
    "You can use one of the following tools for this task: https://www.acmesystems.it/python_http, http.server.ThreadingHTTPServer (3.7+) https://docs.python.org/3/library/http.server.html#http.server.SimpleHTTPRequestHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started httpserver on port  8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [24/Jan/2020 22:30:38] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2020 22:30:38] \"GET /favicon.ico HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2020 22:30:51] \"GET /?q=iranian HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [24/Jan/2020 22:30:51] \"GET /favicon.ico HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C received, shutting down the web server\n"
     ]
    }
   ],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import urllib.parse as urlparse\n",
    "from urllib.parse import parse_qs\n",
    "import json\n",
    "\n",
    "PORT_NUMBER = 8080\n",
    "\n",
    "\n",
    "class MyHandler(BaseHTTPRequestHandler):\n",
    "    def do_GET(s):\n",
    "        \"\"\"Respond to a GET request.\"\"\"\n",
    "        s.send_response(200)\n",
    "        s.send_header(\"Content-type\", \"text/html\")\n",
    "        s.end_headers()\n",
    "        s.wfile.write(\"<html><head><title>Search Engine</title></head><body>\".encode())               \n",
    "        parsed = urlparse.urlparse(s.path) \n",
    "        params = parse_qs(parsed.query)        \n",
    "        if 'q' in params:\n",
    "            # calculate results\n",
    "            processed_query = QueryProcessing.prepare_query(params['q'][0])\n",
    "            res = QueryProcessing.okapi_scoring(processed_query, doc_lengths, index)\n",
    "            sorted_res = sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "            sorted_res_urls = [(doc_urls[item[0]], item[1]) for item in sorted_res]\n",
    "            # output top ten results\n",
    "            s.wfile.write(json.dumps(dict(sorted_res_urls[:10])).encode())\n",
    "        else:\n",
    "            s.wfile.write(\"smth went wrong, can't find \\'q\\' param\".encode())\n",
    "\n",
    "        s.wfile.write(\"</body></html>\".encode())\n",
    "\n",
    "\n",
    "try:\n",
    "    # Create a web server and define the handler to manage the\n",
    "    # incoming request\n",
    "    server = HTTPServer(('', PORT_NUMBER), MyHandler)\n",
    "    print('Started httpserver on port ', PORT_NUMBER)\n",
    "\n",
    "    # Wait forever for incoming htto requests\n",
    "    server.serve_forever()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('^C received, shutting down the web server')\n",
    "    server.socket.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
