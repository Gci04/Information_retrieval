{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do you store in your Google Drive?\n",
    "\n",
    "Sometimes it can be quite troublesome to crawl web data - for example, when you can't just collect data from web-pages because the authentification to a website is required. Today's tutorial is about a dataset of special type - namely, Google Drive data. You will need to get access to the system using OAuth protocol and download and parse files of different types.\n",
    "\n",
    "Plan. \n",
    "1. Download [this little archive](https://drive.google.com/open?id=1Xji4A_dEAm_ycnO0Eq6vxj7ThcqZyJZR), **unzip** it and place the folder anywhere inside your Google Drive. You should get a subtree of 6 folders with files of different types: presentations, pdf-files, texts, and even code.\n",
    "2. Go to [Google Drive API](https://developers.google.com/drive/api/v3/quickstart/python) documentation, read [intro](https://developers.google.com/drive/api/v3/about-sdk) and learn how to [search for files](https://developers.google.com/drive/api/v3/reference/files/list) and [download](https://developers.google.com/drive/api/v3/manage-downloads) them.\n",
    "3. Learn how to open from python such files as [pptx](https://python-pptx.readthedocs.io/en/latest/user/quickstart.html), pdf, docx or even use generalized libraries like [textract](https://textract.readthedocs.io/en/stable/index.html).\n",
    "4. Build search index (preferably, inverted one) based on the documents you get and learn to retrieve file names (e.g. `at least this file.txt`) in response to a query. Validate your code on the following set of queries (there are documents for each of them!):\n",
    "```\n",
    "segmentation\n",
    "algorithm\n",
    "classifer\n",
    "printf\n",
    "predecessor\n",
    "Шеннон\n",
    "Huffman\n",
    "function\n",
    "constructor\n",
    "machine learning\n",
    "dataset\n",
    "Протасов\n",
    "Protasov\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Access GDrive ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the example of how you can oranize your code - it's fine if you change it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the list of all files that are contained (recursively) in the folder of interest. In my case, I called it `air_oauth_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note: code from the example given from Google Drive API example \n",
    "https://developers.google.com/drive/api/v3/quickstart/python\n",
    "'''\n",
    "from __future__ import print_function\n",
    "import pickle, io, re\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "creds = None\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "# The file token.pickle stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists('token.pickle'):\n",
    "    with open('token.pickle', 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            'credentials.json', SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('token.pickle', 'wb') as token:\n",
    "        pickle.dump(creds, token)\n",
    "\n",
    "service = build('drive', 'v3', credentials=creds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_folder_content(service, folder):\n",
    "    res = []\n",
    "    query_str = \"('{}' in parents)\".format(folder[\"id\"])\n",
    "    files_in_folder = service.files().list(q=query_str, spaces='drive',fields=\"nextPageToken, files(id, name , mimeType)\").execute()\n",
    "    files_in_folder.get('files',[])\n",
    "    return files_in_folder[\"files\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gdrive_get_all_files_in_folder(service,folder_name):\n",
    "    query_str = \"(name = '{}')\".format(folder_name)\n",
    "    \n",
    "    results = service.files().list(\n",
    "        q = query_str,spaces='drive',\n",
    "        fields=\"nextPageToken, files(id, name)\").execute()\n",
    "    \n",
    "    folder = results.get('files',[])\n",
    "\n",
    "    if not folder:\n",
    "        print('{} : does not exist in Drive'.format(folder_name))\n",
    "        return []\n",
    "    else:\n",
    "        result_files = []\n",
    "        folders = []\n",
    "        \n",
    "        for item in extract_folder_content(service,folder[0]):\n",
    "            if item[\"mimeType\"] == 'application/vnd.google-apps.folder': folders.append(item) \n",
    "            else : result_files.append(item)    \n",
    "            \n",
    "        while len(folders) != 0: #Go through all the sub-folders id any and collect any kind of document in it\n",
    "            child_folder = folders.pop()\n",
    "            for item in extract_folder_content(service,child_folder):\n",
    "                if item[\"mimeType\"] == 'application/vnd.google-apps.folder': folders.append(item) \n",
    "                else : result_files.append(item) \n",
    "            \n",
    "        return result_files\n",
    "\n",
    "def gdrive_download_file(service,file, path_to_save): \n",
    "    #TODO download file and save it under the path\n",
    "    request = service.files().get_media(fileId=file.get(\"id\"))\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "    \n",
    "    filename = os.path.join(path_to_save, file.get('name'))\n",
    "    f = open(filename, 'wb')\n",
    "    f.write(fh.getvalue())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_of_interest = 'data'\n",
    "files = gdrive_get_all_files_in_folder(service,folder_of_interest)\n",
    "\n",
    "!mkdir \"test_files\"\n",
    "\n",
    "test_dir = \"test_files\"\n",
    "for item in files:\n",
    "    gdrive_download_file(service,item, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tests ##\n",
    "Please fill free to change function signatures and behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_files: 34\n",
      "file here means id, name and type e.g.:  {'id': '1hOd7eR7kdvSxm7Jz8G8ZgXUjNAalahM0', 'name': 'dsa.pdf', 'mimeType': 'application/pdf'}\n"
     ]
    }
   ],
   "source": [
    "assert len(files) == 34, 'Number of files is incorrect'\n",
    "print('n_files:', len(files))\n",
    "\n",
    "print(\"file here means id, name and type e.g.: \", files[0])\n",
    "\n",
    "gdrive_download_file(service,files[0], '.')\n",
    "\n",
    "import os.path\n",
    "assert os.path.isfile(os.path.join('.', files[0][\"name\"])), \"File is not downloaded correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read files ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write here the code to extract text from the files you just downoaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tika\n",
    "from tika import parser\n",
    "\n",
    "def get_file_strings(path):\n",
    "    #TODO change this function to handle different data types properly - textract is not able to parse everything\n",
    "    # Take care of non-text data too\n",
    "    text = None\n",
    "    if path.lower().endswith(tuple(['.mp3', '.mpg', '.mp2', '.mpeg', '.mp4', '.m4p', '.m4v', '.avi', '.wmv', '.png','.jpeg','.gif', '.esp', '.svg'])):\n",
    "        return None\n",
    "    try :\n",
    "        file_data = parser.from_file(path) # Get files text content\n",
    "        if file_data[\"content\"] is not None: \n",
    "            text = str(file_data['content'])\n",
    "            text = \" \".join(re.split(\"[^а-яА-Я||a-zA-Z]+\",text))\n",
    "            return \" \".join(text.split())\n",
    "        else : \n",
    "            return None\n",
    "    except : \n",
    "        print(\"Skipped : {}\".format(path))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dictionary of parsed files\n",
    "files_data = dict()\n",
    "for file in os.scandir(test_dir):    \n",
    "    strings = get_file_strings(file.path)\n",
    "    if strings is not None:\n",
    "        files_data[file.name] = strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tests ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "# Here i changed the last test because of the way i do preprocessing of text in get_file_strings method \n",
    "#Note : I think its better to stick to one language (not mix russian and english)\n",
    "assert len(files_data) == 31\n",
    "print(len(files_data))\n",
    "\n",
    "assert \"Protasov\" in get_file_strings(os.path.join(test_dir, 'at least this file.txt')), \"TXT File parsed incorrectly\"\n",
    "assert \"A Image classification\" in get_file_strings(os.path.join(test_dir, 'deep-features-scene (1).pdf')), \"PDF File parsed incorrectly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Index and search ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Build a search index based on files you just parsed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Preprocessor to prepare document text for querying <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = stopwords.words('russian') + stopwords.words('english')\n",
    "        self.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "\n",
    "    # word tokenize text using nltk lib\n",
    "    def tokenize(self, text):\n",
    "        return nltk.word_tokenize(text)\n",
    "\n",
    "\n",
    "    # stem word using provided stemmer\n",
    "    def stem(self, word, stemmer):\n",
    "        return stemmer.stem(word)\n",
    "\n",
    "\n",
    "    # check if word is appropriate - not a stop word and isalpha, \n",
    "    # i.e consists of letters, not punctuation, numbers, dates\n",
    "    def is_apt_word(self, word):\n",
    "        return word not in self.stop_words and word.isalpha()\n",
    "\n",
    "\n",
    "    # combines all previous methods together\n",
    "    # tokenizes lowercased text and stems it, ignoring not appropriate words\n",
    "    def preprocess(self, text):\n",
    "        tokenized = self.tokenize(text.lower())\n",
    "        return [self.stem(w, self.ps) for w in tokenized if self.is_apt_word(w) and len(w) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build inverted index with the documents from drive\n",
    "from collections import Counter\n",
    "def build_inverted_index(files_data):\n",
    "    \"\"\"returns inverted list, document lenghts and documents names map. \n",
    "    Its for okapi_scoring (boolean retrieval can be used) -- Implementation from previous Labs\"\"\"\n",
    "    \n",
    "    prep = Preprocessor()\n",
    "    doc_lengths = {}\n",
    "    inverted_list = {}\n",
    "    doc_names = {}\n",
    "    \n",
    "    for doc_id , doc in enumerate(files_data.items()):\n",
    "        file_content = prep.preprocess(doc[1])\n",
    "        doc_lengths[doc_id] = len(file_content)\n",
    "        doc_names[doc_id] = doc[0]\n",
    "        \n",
    "        article_index = Counter(file_content)\n",
    "        \n",
    "        for term in article_index.keys():\n",
    "            article_freq = article_index[term]\n",
    "            if term not in inverted_list:                \n",
    "                inverted_list[term] = [article_freq, (doc_id, article_freq)]\n",
    "            else:\n",
    "                inverted_list[term][0] += article_freq\n",
    "                inverted_list[term].append((doc_id, article_freq))\n",
    "                \n",
    "    return [inverted_list, doc_lengths, doc_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index, doc_lengths, doc_names = build_inverted_index(files_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def find(query, index, doc_lengths, k1=1.2, b=0.75):\n",
    "    scores = {}\n",
    "    N = len(doc_lengths)\n",
    "    avgdl = sum(doc_lengths.values()) / float(len(doc_lengths))\n",
    "    for term in query.keys():\n",
    "        if term not in index:  # ignoring absent terms\n",
    "            continue\n",
    "        n_docs = len(index[term]) - 1\n",
    "        idf = math.log10((N - n_docs + 0.5) / (n_docs + 0.5))\n",
    "        postings = index[term][1:]\n",
    "        for posting in postings:\n",
    "            doc_id = posting[0]\n",
    "            doc_tf = posting[1]\n",
    "            score = idf * doc_tf * (k1 + 1) / (doc_tf + k1 * (1 - b + b * (doc_lengths[doc_id] / avgdl)))\n",
    "            if doc_id not in scores:\n",
    "                scores[doc_id] = score\n",
    "            else:  # accumulate scores\n",
    "                scores[doc_id] += score\n",
    "    return list(scores.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tests ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"segmentation\", \"algorithm\", \"printf\", \"predecessor\", \"Huffman\",\n",
    "           \"function\", \"constructor\", \"machine learning\", \"dataset\", \"Protasov\"]\n",
    "prep = Preprocessor()\n",
    "\n",
    "for query in queries:\n",
    "    query = Counter(prep.preprocess(query))\n",
    "    r = find(query, inverted_index,doc_lengths)\n",
    "    r = list(map(lambda x : doc_names.get(x), r))\n",
    "    #print(\"Results for: \", query)\n",
    "    #print(\"\\t\", r)\n",
    "    assert len(r) > 0, \"Query should return at least 1 document\"\n",
    "    assert len(r) > 1, \"Query should return at least 2 documents\"\n",
    "    assert \"at least this file.txt\" in r, \"This file has all the queries. It should be in a result\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
