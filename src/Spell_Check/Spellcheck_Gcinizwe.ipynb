{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorri not veri gud in inglish\n",
    "\n",
    "Have you ever googled someone's name without knowing exactly how should it be written? Were you ever reluctant to look up the correct spelling of a query you typed? Or just unable to type properly because of being in a rush? Modern search engines usually do a pretty good job in deciphering defective user input. In order to be able to do that, a good spell-checking mechanism should be incorporated into a search procedure. Today we will take one step further towards building a good search engine and work on tolerant retrieval with respect to user queries. We will consider two cases:\n",
    "\n",
    "1. User knows that he doesn't know the correct spelling OR he wants to get the results that follow some known pattern, so he uses so called wildcards - queries like 'retr*val';\n",
    "2. User doesn't know the correct spelling OR he doesn't care OR he's in a rush OR he expects his mistakes will be corrected OR your option, so he makes mistakes and we need to handle them using:\n",
    "\n",
    "    2.1. Simple spellchecker by Peter Norvig;\n",
    "    \n",
    "    2.2. Phonetic correction by means of Soundex algorithm;\n",
    "    \n",
    "    2.3. Trigrams with Jaccard coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Handling wildcards\n",
    "\n",
    "We will handle wildcard queries using k-grams. K-grams is a list of consecutive k chars in a string - i.e., for the word *'star'*, it will be '*\\$st*', '*sta*', '*tar*', and '*ar$*', if we take k=3. Take a look at [book](https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf) *chapter 3.2.2* to understand how k-grams can help efficiently match a wildcard against dictionary words. Here we will only consider wildcards with star symbols (may be multiple).\n",
    "\n",
    "Notice that for building k-grams index, **we will need a vocabulary of original word forms** to compare words in user input to the vocabulary of \"correct\" words (think why inverted index which we built for stemmed words doesn't work here).   \n",
    "\n",
    "You need to implement the following:\n",
    "\n",
    "- `build_inverted_index_orig_forms` - creates inverted index of original world forms from `facts` list, which is already given to you.  \n",
    "    Output format: `term:[collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]`\n",
    "    \n",
    "\n",
    "- `build_k_gram_index` - creates k-gram index which maps every k-gram encountered in facts collection to a list of words containing this k-gram. Use the abovementioned inverted index of original words to construct this index.  \n",
    "    Output format: `'k_gram': ['word1_with_k_gram', 'word2_with_k_gram', ...]`\n",
    "    \n",
    "    \n",
    "- `generate_wildcard_options` - produce a list of vocabulary words matching given wildcard by intersecting postings of k-grams present in the wildcard (refer to *ch 3.2.2*). \n",
    "\n",
    "- `search_wildcard` - return list of facts that contain the words matching a wildcard query.\n",
    "\n",
    "\n",
    "We will use the dataset with curious facts for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women have twice as many pain receptors on their body than men. But a much higher pain tolerance.\n",
      "There are more stars in space than there are grains of sand on every beach in the world.\n",
      "For every human on Earth there are 1.6 million ants.\n",
      "The total weight of all those ants, however, is about the same as all the humans.\n",
      "On Jupiter and Saturn it rains diamonds.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "data_url = \"https://raw.githubusercontent.com/hsu-ai-course/hsu.ai/master/code/datasets/nlp/facts.txt\"\n",
    "file_name= \"facts.txt\"\n",
    "# urllib.request.urlretrieve(data_url, file_name)\n",
    "\n",
    "facts = []\n",
    "with open(file_name,encoding=\"windows-1251\") as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "        facts.append(line.strip('\\n')[len(str(cnt))+2:])\n",
    "print(*facts[-5:], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import re\n",
    "from itertools import islice, tee\n",
    "\n",
    "def build_inverted_index_orig_forms(documents):\n",
    "    #TODO build an inverted index of original word forms \n",
    "    # (without stemming, just word tokenized and lowercased)   \n",
    "    inverted_index = {}\n",
    "    for doc_id ,doc in enumerate(documents,0):\n",
    "        words = nltk.word_tokenize(doc.lower())\n",
    "        count = Counter(words)\n",
    "        for pair in count.items():\n",
    "            if pair[0] not in inverted_index.keys():\n",
    "                inverted_index[pair[0]] = [pair[1],(doc_id,pair[1])]\n",
    "            else:\n",
    "                inverted_index[pair[0]][0] += pair[1]\n",
    "                inverted_index[pair[0]].append((doc_id,pair[1]))\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "def build_k_gram_index(inverted_index, k):\n",
    "    #TODO build index of k-grams for dictionary words. \n",
    "    # Padd with '$' ($word$) before splitting to k-grams    \n",
    "    k_gram_index = {}\n",
    "    padded_words = [\"$\"+i+\"$\" for i in inverted_index.keys()]\n",
    "    all_words = list(inverted_index.keys())\n",
    "    \n",
    "    for padded_w in tqdm.tqdm_notebook(padded_words):\n",
    "        #k_grams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(padded_w, k))))\n",
    "        k_grams = nltk.ngrams(padded_w, k)\n",
    "        for k_gram in k_grams:\n",
    "            k_gram = \"\".join(k_gram)\n",
    "            res_words_with_kgram = [i[1:-1] for i in padded_words if k_gram in i ]\n",
    "            k_gram_index[k_gram] = res_words_with_kgram\n",
    "    \n",
    "    return k_gram_index\n",
    "\n",
    "def generate_wildcard_options(wildcard, k_gram_index,k=3): \n",
    "    \n",
    "    if (wildcard[0] == \"*\"): wildcard = wildcard+\"$\"\n",
    "    elif (wildcard[-1] == \"*\"): wildcard = \"$\"+wildcard\n",
    "    else : wildcard = \"$\"+wildcard+\"$\"\n",
    "    \n",
    "    wildcard_splits = wildcard.split(\"*\")\n",
    "\n",
    "    k = len(list(k_gram_index.keys())[0]) #3\n",
    "    kgrams = list(k_gram_index.keys())\n",
    "    answer = None\n",
    "    for index, w_c in enumerate(wildcard_splits):\n",
    "        if len(w_c) < 3:\n",
    "            if index + 1 < len(wildcard_splits):\n",
    "                while len(w_c) < k:\n",
    "                    w_c += '.'\n",
    "            else:\n",
    "                while len(w_c) < k:\n",
    "                    w_c = '.' + w_c\n",
    "                    \n",
    "        for i in range(len(w_c) - k + 1):\n",
    "            temp_res = set()\n",
    "            kgram = w_c[i:i + k]\n",
    "            kgram = kgram.replace(\"$\", r\"\\$\")\n",
    "            rexpr = re.compile(kgram)\n",
    "            results = filter(rexpr.match, kgrams)\n",
    "            \n",
    "            for item in results:\n",
    "                temp_res = temp_res.union(set(k_gram_index[item]))\n",
    "            if answer is None:\n",
    "                answer = temp_res\n",
    "            answer = answer.intersection(temp_res)\n",
    "\n",
    "    n = len(re.sub(\"[*$]\",\"\",wildcard))\n",
    "\n",
    "    return list(filter(lambda w : n <= len(w),answer))\n",
    "\n",
    "def search_wildcard(wildcard, k_gram_index, index, docs):\n",
    "    \"\"\"retrive list of documnets (facts) that contain words matching wildcard\"\"\"\n",
    "    \n",
    "    wildcard_options = generate_wildcard_options(wildcard,k_gram_index)\n",
    "    results = []\n",
    "    for option in wildcard_options:\n",
    "        #get the documents containing this word\n",
    "        doc_list = index.get(option, None)\n",
    "        if doc_list != None:\n",
    "            results += [docs[i[0]] for i in doc_list[1:]]\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reduced', 'received', 'recorded']\n",
      "A person can live without food for about a month, but only about a week without water. If the amount of water in your body is \u001b[1m\u001b[91mreduced\u001b[0m by just 1%, you’ll feel thirsty. If it’s \u001b[1m\u001b[91mreduced\u001b[0m by 10%, you’ll die.\n",
      "More than 50% of the people in the world have never made or \u001b[1m\u001b[91mreceived\u001b[0m a telephone call.\n",
      "The largest \u001b[1m\u001b[91mrecorded\u001b[0m snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n"
     ]
    }
   ],
   "source": [
    "index_orig_forms = build_inverted_index_orig_forms(facts)\n",
    "k_gram_index = build_k_gram_index(index_orig_forms, 3)\n",
    "\n",
    "wildcard = \"re*ed\"\n",
    "\n",
    "# wildcard_options = generate_wildcard_options(wildcard, k_gram_index, index_orig_forms)\n",
    "wildcard_options = generate_wildcard_options(wildcard, k_gram_index)\n",
    "print(wildcard_options)\n",
    "assert(len(wildcard_options) >= 3)\n",
    "\n",
    "wildcard_results = search_wildcard(wildcard, k_gram_index, index_orig_forms, facts)\n",
    "# some pretty printing\n",
    "for r in wildcard_results:\n",
    "    # highlight terms for visual evaluation\n",
    "    for term in wildcard_options:\n",
    "        r = re.sub(r'(' + term + ')', r'\\033[1m\\033[91m\\1\\033[0m', r, flags=re.I)\n",
    "    print(r)\n",
    "\n",
    "assert(len(wildcard_results) >=3)\n",
    "\n",
    "assert \"James Buchanan, the 15th U.S. president continuously bought slaves with his own money in order to free them.\" in search_wildcard(\"pres*dent\", k_gram_index, index_orig_forms, facts)\n",
    "assert \"9 out of 10 Americans are deficient in Potassium.\" in search_wildcard(\"p*tas*um\", k_gram_index, index_orig_forms, facts)\n",
    "assert \"A man from Britain changed his name to Tim Pppppppppprice to make it harder for telemarketers to pronounce.\" in search_wildcard(\"*price\", k_gram_index, index_orig_forms, facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset \n",
    "\n",
    "Download github typo dataset from [here](https://github.com/mhagiwara/github-typo-corpus).\n",
    "Load it with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -o \"github-typo-corpus.v1.0.0.jsonl.gz\" \"https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\"\n",
    "# !gzip -d \"github-typo-corpus.v1.0.0.jsonl.gz\"\n",
    "\n",
    "# !wget https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\n",
    "# !gunzip -k ./github-typo-corpus.v1.0.0.jsonl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 245909\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install jsonlines\n",
    "import jsonlines\n",
    "\n",
    "dataset_file = \"github-typo-corpus.v1.0.0.jsonl\"\n",
    "\n",
    "dataset = []\n",
    "other_langs = set()\n",
    "\n",
    "with jsonlines.open(dataset_file) as reader:\n",
    "    for obj in reader:\n",
    "        for edit in obj['edits']:\n",
    "            if edit['src']['lang'] != 'eng':\n",
    "                other_langs.add(edit['src']['lang'])\n",
    "                continue\n",
    "\n",
    "            if edit['is_typo']:\n",
    "                src, tgt = edit['src']['text'], edit['tgt']['text']\n",
    "                if src.lower() != tgt.lower():\n",
    "                    dataset.append((edit['src']['text'], edit['tgt']['text']))\n",
    "                \n",
    "print(f\"Dataset size = {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore sample typos\n",
    "Please, explore the dataset. You may see, that this is\n",
    "- mostly markdown\n",
    "- some common mistakes with do/does\n",
    "- some just refer to punctuation typos (which we do not consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \"\"\"Make am instance. =>         \"\"\"Make an instance.\n",
      "* travis: test agains Node.js 11 => * travis: test against Node.js 11\n",
      "The parser receive a string and returns an array inside a user-provided  => The parser receives a string and returns an array inside a user-provided \n",
      "CSV data is send through the `write` function and the resulted data is obtained => CSV data is sent through the `write` function and the resulting data is obtained\n",
      "One useful function part of the Stream API is `pipe` to interact between  => One useful function of the Stream API is `pipe` to interact between \n",
      "source to a `stream.Writable` object destination. This example available as  => source to a `stream.Writable` object destination. This example is available as \n",
      "`node samples/pipe.js` read the file, parse its content and transform it. => `node samples/pipe.js` and reads the file, parses its content and transforms it.\n",
      "Most of the generator is imported from its parent project [CSV][csv] in a effort  => Most of the generator is imported from its parent project [CSV][csv] in an effort \n",
      "*   `quote`             Optionnal character surrounding a field, one character only, defaults to double quotes.    => *   `quote`             Optional character surrounding a field, one character only, defaults to double quotes.   \n",
      "The parser receive a string and return an array inside a user-provided  => The parser receive a string and returns an array inside a user-provided \n"
     ]
    }
   ],
   "source": [
    "for pair in dataset[1010:1020]:\n",
    "    print(f\"{pair[0]} => {pair[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a dataset vocabulary\n",
    "We will need it for Norvig's spellchecker as well as for estimating overall correction quality. Consider only word-level. Be carefull, there is markdown (e.g. \\`name\\`. \\[url\\]\\(http://url)) and comment symbols (\\#, //, \\*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sent):\n",
    "    # splits sentence to words, filtering out non-alphabetical terms\n",
    "    #words = nltk.word_tokenize(re.sub(\"[^a-zA-Z0-9]+\",\" \",sent))\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    words_filtered = filter(lambda x: x.isalpha(), words)\n",
    "    return words_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 63204 words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for pair in dataset:\n",
    "    for word in sent_to_words(pair[1].lower()):\n",
    "        vocabulary[word] += 1\n",
    "print(f'Vocabulary has {len(vocabulary)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 76802 words\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for pair in dataset:\n",
    "    for word in sent_to_words(pair[1].lower()):\n",
    "        vocabulary[word] += 1\n",
    "print(f'Vocabulary has {len(vocabulary)} words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('function', 6162), ('de', 82), ('deutsch', 4), ('nocomments', 2), ('you', 42018), ('can', 26006), ('disable', 529), ('comments', 359), ('for', 44724), ('the', 206931)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "print(list(islice(vocabulary.items(), 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Implement context-independent spellcheker ##\n",
    "\n",
    "1. Write code to compute editorial distance\n",
    "\n",
    "1. [Norvig's corrector](https://norvig.com/spell-correct.html)\n",
    "\n",
    "1. [Soundex](https://en.wikipedia.org/wiki/Soundex)\n",
    "\n",
    "1. Trigrams with Jaccard coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editorial distance\n",
    "\n",
    "Frequently used distance measure between two character sequences. We will use this distance to sort Soundex search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_dist1(s1, s2) -> int:\n",
    "    \"\"\"REF : https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance\"\"\"\n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1\n",
    "    res_distances = range(len(s1) + 1)\n",
    "    \n",
    "    for index_w2,char_w2 in enumerate(s2):\n",
    "        temp_distances = [index_w2+1]\n",
    "        \n",
    "        for index_w1,char_w1 in enumerate(s1):\n",
    "            if char_w1 == char_w2 : temp_distances.append(res_distances[index_w1])\n",
    "            else : temp_distances.append(1 + min((temp_distances[-1],res_distances[index_w1], res_distances[index_w1+1])))\n",
    "            \n",
    "        res_distances = temp_distances\n",
    "        \n",
    "    return res_distances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_dist(s1, s2) -> int:\n",
    "    \"\"\"Recursive version of Levenshtein distance\"\"\"\n",
    "    if s1 == \"\":\n",
    "        return len(s2)\n",
    "    if s2 == \"\":\n",
    "        return len(s1)\n",
    "    if s1[-1] == s2[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "    \n",
    "    return min((edit_dist(s1[:-1], s2) + 1,edit_dist(s1, s2[:-1]) + 1, edit_dist(s1[:-1], s2[:-1]) + cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert edit_dist(\"korrectud\", \"corrected\") == 2, \"Edit distance is computed incorrectly\"\n",
    "assert edit_dist(\"soem\", \"some\") == 2, \"Edit distance is computed incorrectly\"\n",
    "assert edit_dist(\"one\", \"one\") == 0, \"Edit distance is computed incorrectly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norvig's spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From : https://norvig.com/spell-correct.html \n",
    "\n",
    "def candidates(word): \n",
    "    \"possible spelling corrections for word\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    return set(w for w in words if w in vocabulary)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from given word\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def fix_typo_norvig(word, vocabulary) -> str:\n",
    "    n = sum(vocabulary.values())\n",
    "    \n",
    "    return max(candidates(word), key=lambda w : vocabulary[w] / n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert fix_typo_norvig(\"korrectud\",vocabulary) == \"corrected\", \"Norvig's correcter doesn't work\"\n",
    "assert fix_typo_norvig(\"speling\",vocabulary) == \"spelling\", \"Norvig's correcter doesn't work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soundex \n",
    "\n",
    "For cases when the exact spelling is unknown, phonetic algorithms such as Soundex can be very helpful - they allow user to type a word the way he thinks it should sound, and then suggest the corrrect version. Go through *chapter 3.4* to understand how Soundex algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def produce_soundex_code(word):\n",
    "    \"\"\"Generate soundex code for any given word\"\"\"\n",
    "    #Retain the first letter of the name added to result\n",
    "    result = word[0].upper()\n",
    "    \n",
    "    #part of step 1 , Avoid removing vowels immediately \n",
    "    word = re.sub('[hw]', '', word, flags=re.I).lower()\n",
    "\n",
    "    # Step 2 & 3 : mapping letters to respective number code  \n",
    "    coded_word = re.sub('[bfpv]+', '1', word)\n",
    "    coded_word = re.sub('[cgjkqsxz]+', '2', coded_word)\n",
    "    coded_word = re.sub('[dt]+', '3', coded_word)\n",
    "    coded_word = re.sub('l+', '4', coded_word)\n",
    "    coded_word = re.sub('[mn]+', '5', coded_word)\n",
    "    coded_word = re.sub('r+', '6', coded_word)\n",
    "\n",
    "    # Remove the first letter\n",
    "    coded_word = coded_word[1:]\n",
    "\n",
    "    # remove all vowels and y's now\n",
    "    coded_word = re.sub('[yaeiou]','', coded_word)\n",
    "\n",
    "    # Result should be made of the first letter plus 3 other codes\n",
    "    result += coded_word[0:3] if len(coded_word) > 3 else (coded_word + ('0'*(3-len(coded_word))))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_soundex_index(dictionary):\n",
    "    \"\"\"build soundex index for dictionary words.\n",
    "    Input : vocabulary dictionary of original words\n",
    "    Output : dictionary -> {'code1': ['word1_with_code1', 'word2_with_code1', ...]  } \n",
    "    \"\"\"\n",
    "    soundex_index = {}\n",
    "    \n",
    "    for w in dictionary.keys():\n",
    "        word_code = produce_soundex_code(w)\n",
    "        if word_code not in soundex_index:\n",
    "            soundex_index[word_code] = [w]\n",
    "        else:\n",
    "            soundex_index[word_code].append(w)\n",
    "    \n",
    "    return soundex_index\n",
    "\n",
    "def fix_typo_soundex(word, soundex_index) -> list:\n",
    "    \"\"\" \n",
    "    retrieve words from vocabulary that match with result by soundex fingerprint\n",
    "    NB : ordered results by editorial distance\n",
    "    \"\"\"\n",
    "    word_code = produce_soundex_code(word)\n",
    "    options = soundex_index.get(word_code,None)\n",
    "    if options == None: return []\n",
    "    else: return sorted(options,key= lambda x : edit_dist(x,word))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B635 B635\n",
      "['enough', 'ensue', 'eng', 'enjoy', 'emoji', 'enqueue', 'ens', 'enc', 'emojii', 'enki', 'enso', 'enzo', 'enwiki', 'emesh', 'emg', 'emacs', 'emc', 'emas', 'euank', 'enmasse', 'emac', 'emmc', 'emgo']\n"
     ]
    }
   ],
   "source": [
    "soundex_index = build_soundex_index(vocabulary)\n",
    "\n",
    "code1 = produce_soundex_code(\"britney\")\n",
    "code2 = produce_soundex_code(\"breatany\")\n",
    "print(code1, code2)\n",
    "assert code1 == code2\n",
    "\n",
    "print(fix_typo_soundex(\"enouhg\", soundex_index))\n",
    "assert \"enough\" in fix_typo_soundex(\"enouhg\", soundex_index), \"Assert soundex failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams with Jaccard coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_typo_kgram(word, k_gram_index) -> list:\n",
    "    #TODO return best matches with respect to Jaccard index\n",
    "    k = len(k_gram_index.keys()[0])\n",
    "    k_grams = nltk.ngrams(\"$\"+word+\"$\", k)\n",
    "    res = []\n",
    "    for k_gram in k_grams:\n",
    "        k_gram = \"\".join(k_gram)\n",
    "        res += k_gram_index.get(k_gram,[])\n",
    "    if len(res) != 0 : return sorted(res,key= lambda w : nltk.jaccard_distance(set(word), set(w)))\n",
    "    else: return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_gram_index_github = build_k_gram_index(vocabulary, 3)\n",
    "print(fix_typo_kgram(\"enouh\", k_gram_index_github)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enough', 'enought', 'eno', 'endogenous', 'enomem', 'enospc', 'enosys', 'enormous', 'renounce', 'en', 'exogenous', 'uh', 'enormously', 'homogenous', 'env', 'end', 'huh', 'hetrogenous', 'enh', 'ens']\n"
     ]
    }
   ],
   "source": [
    "k_gram_index_github = build_k_gram_index(vocabulary, 3)\n",
    "print(fix_typo_kgram(\"enouh\", k_gram_index_github)[:20])\n",
    "assert \"enough\" in fix_typo_kgram(\"enouh\", k_gram_index_github), \"Assert k-gram failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Estimate quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norvig, soundex, kgram = 0, 0, 0\n",
    "limit = 10000\n",
    "counter = limit\n",
    "for i, (src, target) in enumerate(dataset):\n",
    "    if i == limit:\n",
    "        break\n",
    "    words = sent_to_words(src.lower())\n",
    "    # word suspected for typos\n",
    "    sn, ss, sk = src.lower(), src.lower(), src.lower()\n",
    "    for word in words:\n",
    "        if word not in vocabulary and word.isalpha():\n",
    "            # top-1 accuracy\n",
    "            wn, ws, wk = fix_typo_norvig(word), \\\n",
    "                         fix_typo_soundex(word, soundex_index)[0], \\\n",
    "                         fix_typo_kgram(word, k_gram_index_github)[0]\n",
    "            sn = sn.replace(word, wn)\n",
    "            ss = ss.replace(word, ws)\n",
    "            sk = sk.replace(word, wk)\n",
    "    norvig += int(sn == target.lower())\n",
    "    soundex += int(ss == target.lower())\n",
    "    kgram += int(sk == target.lower())\n",
    "\n",
    "print(f\"Norvig accuracy ({norvig}) = {norvig / limit}\")\n",
    "print(f\"Soundex accuracy ({soundex}) = {soundex / limit}\")\n",
    "print(f\"k-gram accuracy ({kgram}) = {kgram / limit}\")\n",
    "\n",
    "# Norvig accuracy (2346) = 0.2346\n",
    "# Soundex accuracy (1673) = 0.1673\n",
    "# k-gram accuracy (1566) = 0.1566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm github-typo-corpus.v1.0.0.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
